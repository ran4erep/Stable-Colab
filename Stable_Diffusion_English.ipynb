{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ran4erep/Stable-Colab/blob/main/Stable_Diffusion_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7l39D9u2_De"
      },
      "source": [
        "↑ It is recommended to click File --> Save a copy on Disk\n",
        "\n",
        "This is needed to keep all the data you have entered. For example, the token for CivitAI site, so you don't have to copy and paste it into the corresponding form every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKe1AR4FFMr9"
      },
      "outputs": [],
      "source": [
        "# @title Stable Diffusion Installing and connecting to Google Drive: { vertical-output: true, form-width: \"10%\", display-mode: \"form\" }\n",
        "# @markdown Do you wish to use Google Drive?\n",
        "use_gdrive = False # @param {type:\"boolean\"}\n",
        "# @markdown\n",
        "\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "import sys\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import display, update_display\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from urllib.parse import unquote\n",
        "\n",
        "if use_gdrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  print(\"GDrive sucessfully mounted to /content/gdrive\")\n",
        "  # Создаём папку в Google Drive, если её нет\n",
        "  if not os.path.exists(\"/content/gdrive/MyDrive/SDOutput\"):\n",
        "    os.makedirs(\"/content/gdrive/MyDrive/SDOutput\", exist_ok=True)\n",
        "    print(\"Folder successfully created /content/gdrive/MyDrive/SDOutput\")\n",
        "\n",
        "#!pip install diffusers[\"torch\"] transformers\n",
        "!pip install diffusers\n",
        "!pip install accelerate\n",
        "!pip install compel\n",
        "#!pip install git+https://github.com/huggingface/diffusers\n",
        "#!pip install compel --upgrade\n",
        "\n",
        "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image, AutoPipelineForInpainting\n",
        "from diffusers import (\n",
        "    DDPMScheduler,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        ")\n",
        "from diffusers.utils import make_image_grid, load_image\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "import transformers\n",
        "used_models_array = [\"\"]\n",
        "\n",
        "previous_checkpoint = None\n",
        "previous_lora_path = None\n",
        "pipe = None\n",
        "previous_mode = None\n",
        "downloaded_model_name = None\n",
        "downloaded_model_basename = None\n",
        "downloaded_lora_name = None\n",
        "downloaded_lora_basename = None\n",
        "current_url = None\n",
        "previous_url = None\n",
        "lora_url = None\n",
        "previous_lora_url = None\n",
        "civitai_models = []\n",
        "previous_clip_skip = None\n",
        "\n",
        "clear_output()\n",
        "print(\"Installation is complete :)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkHQj8AHJKzr"
      },
      "outputs": [],
      "source": [
        "# @title Stable Diffusion preparing: { vertical-output: true, form-width: \"10%\", display-mode: \"form\" }\n",
        "# @markdown\n",
        "# @markdown Models to be loaded into the script:\n",
        "# @markdown\n",
        "# @markdown [CivitAI](https://civitai.com/models)\n",
        "# @markdown\n",
        "# @markdown [HuggingFace](https://huggingface.co/models?library=diffusers)\n",
        "\n",
        "#!nvidia-smi\n",
        "\n",
        "####################\n",
        "# Список опробованных мною моделей:\n",
        "# PicX_real:       GraydientPlatformAPI/picx-real\n",
        "# Juggernaut:      digiplay/Juggernaut_final\n",
        "# Juggernaut XL:   frankjoshua/juggernautXL_version6Rundiffusion\n",
        "# RealVisXL 3.0:   SG161222/RealVisXL_V3.0_Turbo\n",
        "# Base 1.5:        runwayml/stable-diffusion-v1-5\n",
        "# Base SDXL:       stabilityai/stable-diffusion-xl-base-1.0\n",
        "# Base SDXL Turbo: stabilityai/sdxl-turbo\n",
        "####################\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Your token from the CivitAI website (optional for downloading models from CivitAI that require registration on the website.\n",
        "# @markdown\n",
        "# @markdown You can get it in the settings of your CivitAI account, in the API Keys block):\n",
        "civitai_token = \"\" # @param {type:\"string\"}\n",
        "# @markdown Модель для загрузки:\n",
        "current_checkpoint = \"\" # @param [\"runwayml/stable-diffusion-v1-5\", \"stabilityai/stable-diffusion-xl-base-1.0\", \"runwayml/stable-diffusion-inpainting\", \"stabilityai/sdxl-turbo\", \"digiplay/Juggernaut_final\", \"GraydientPlatformAPI/picx-real\", \"frankjoshua/juggernautXL_version6Rundiffusion\", \"SG161222/RealVisXL_V3.0_Turbo\", \"SG161222/Realistic_Vision_V6.0_B1_noVAE\", \"stablediffusionapi/lob-realvisxl-v20\"] {allow-input: true}\n",
        "# @markdown Use safetensors?\n",
        "use_safetensors = True # @param [\"True\", \"False\", \"None\"] {type:\"raw\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Use LoRA?\n",
        "use_lora = False # @param {type:\"boolean\"}\n",
        "# @markdown Current LoRA:\n",
        "lora_path = \"\" # @param [\"nerijs/pixel-art-xl\", \"ntc-ai/SDXL-LoRA-slider.cinematic-lighting\"] {allow-input: true}\n",
        "# @markdown LoRA weights name:\n",
        "weight_name = \"\" # @param [\"pixel-art-xl.safetensors\", \"cinematic lighting.safetensors\"] {allow-input: true}\n",
        "# @markdown Link to LoRA from website CivitAI:\n",
        "lora_url = \"\" # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "current_device = \"cuda\"\n",
        "# @markdown CLIP skip\n",
        "# @markdown (1 means that all layers are used, this is the default value):\n",
        "clip_skip = 1 # @param {type:\"slider\", min:1, max:12, step:1}\n",
        "## @markdown Вариант:\n",
        "#current_variant = \"fp16\" # @param [\"\", \"fp16\", \"ema\"]\n",
        "\n",
        "display_handle = display(None, display_id=True)\n",
        "\n",
        "def load_civit_model(url):\n",
        "  global current_checkpoint, previous_checkpoint, downloaded_model_name, downloaded_model_basename, previous_url, current_url, civitai_models\n",
        "  for element in civitai_models:\n",
        "    if current_checkpoint in element[0]:\n",
        "      current_checkpoint = element[1]\n",
        "      return\n",
        "\n",
        "  if current_url != previous_url:\n",
        "    if civitai_token:\n",
        "      url = url + \"&token=\" + civitai_token\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
        "    block_size = 100000\n",
        "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "\n",
        "    if 'content-disposition' in response.headers:\n",
        "      content_disposition = response.headers['content-disposition']\n",
        "      filename_index = content_disposition.find('filename=')\n",
        "      if filename_index != -1:\n",
        "        filename = content_disposition[filename_index + len('filename='):]\n",
        "        filename = unquote(filename)\n",
        "        filename = filename.strip('\"')\n",
        "        downloaded_model_name = filename\n",
        "    if not downloaded_model_name:\n",
        "      downloaded_model_name = os.path.basename(url)\n",
        "\n",
        "    downloaded_model_basename, _ = os.path.splitext(downloaded_model_name)\n",
        "    with open(downloaded_model_name, 'wb') as file:\n",
        "      for data in response.iter_content(block_size):\n",
        "        progress_bar.update(len(data))\n",
        "        file.write(data)\n",
        "    progress_bar.close()\n",
        "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "      print(\"Model loading error :(\")\n",
        "\n",
        "    print(\"\\nModel \" + downloaded_model_basename + \" successfully loaded :) \")\n",
        "    print(\"Wait, model conversion is in progress....\\n\")\n",
        "\n",
        "    from pkg_resources import get_distribution\n",
        "    diffusers_version = get_distribution('diffusers').version\n",
        "    lib_url = \"https://raw.githubusercontent.com/huggingface/diffusers/v\" + diffusers_version + \"/scripts/convert_original_stable_diffusion_to_diffusers.py\"\n",
        "    !wget \"$lib_url\"\n",
        "\n",
        "    !python convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$downloaded_model_name\" --dump_path \"$downloaded_model_basename/\" --from_safetensors\n",
        "    previous_url = current_checkpoint\n",
        "    current_checkpoint = downloaded_model_basename\n",
        "    civitai_models.append((url, downloaded_model_basename))\n",
        "    os.remove(downloaded_model_name)\n",
        "    os.remove(\"convert_original_stable_diffusion_to_diffusers.py\")\n",
        "    clear_output()\n",
        "    print(\"Model \" + downloaded_model_basename + \" successfully converted and ready to work :)\")\n",
        "\n",
        "repo_regexp = r'[^\\/\\s]+\\s*\\/\\s*[^\\/\\s]+'\n",
        "link_regexp = r'^https?://\\S+$'\n",
        "\n",
        "if re.match(repo_regexp, current_checkpoint):\n",
        "  current_checkpoint = current_checkpoint\n",
        "elif re.match(link_regexp, current_checkpoint):\n",
        "  current_url = current_checkpoint\n",
        "  load_civit_model(current_checkpoint)\n",
        "else:\n",
        "  print(\"Could not find the model from your link. :( Please provide a link to the HuggingFace repository of the form \\\"author/model_name\\\" or a direct link to the .safetensors file from the CivitAI website.\")\n",
        "\n",
        "def flush():\n",
        "  del(pipe)\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "if pipe and not use_lora:\n",
        "  pipe.unload_lora_weights()\n",
        "\n",
        "\n",
        "def run_chkpt(current_checkpoint, lora_path, use_lora, mode):\n",
        "    global previous_checkpoint, previous_lora_path, previous_mode, lora_url, previous_lora_url, downloaded_lora_basename, downloaded_lora_name, pipe, clip_skip, previous_clip_skip\n",
        "    if current_checkpoint != previous_checkpoint or mode != previous_mode or previous_clip_skip != clip_skip:\n",
        "        used_models_array.append(current_checkpoint)\n",
        "        print(\"Installing the model...\")\n",
        "        if clip_skip > 1:\n",
        "          text_encoder = transformers.CLIPTextModel.from_pretrained(current_checkpoint, subfolder = \"text_encoder\", num_hidden_layers = 12 - (clip_skip - 1), torch_dtype = torch.float16)\n",
        "        if mode == \"txt2img\":\n",
        "          if clip_skip > 1:\n",
        "            pipe = AutoPipelineForText2Image.from_pretrained(current_checkpoint, torch_dtype=torch.float16, use_safetensors=use_safetensors, text_encoder = text_encoder)\n",
        "          else:\n",
        "            pipe = AutoPipelineForText2Image.from_pretrained(current_checkpoint, torch_dtype=torch.float16, use_safetensors=use_safetensors)\n",
        "        if mode == \"img2img\":\n",
        "          if clip_skip > 1:\n",
        "            pipe = AutoPipelineForImage2Image.from_pretrained(current_checkpoint, torch_dtype=torch.float16, use_safetensors=use_safetensors, text_encoder = text_encoder)\n",
        "          else:\n",
        "            pipe = AutoPipelineForImage2Image.from_pretrained(current_checkpoint, torch_dtype=torch.float16, use_safetensors=use_safetensors)\n",
        "        if mode == \"inpainting\":\n",
        "          if clip_skip > 1:\n",
        "            pipe = AutoPipelineForInpainting.from_pretrained(current_checkpoint, torch_dtype=torch.float16, use_safetensors=use_safetensors, text_encoder = text_encoder)\n",
        "          else:\n",
        "            pipe = AutoPipelineForInpainting.from_pretrained(current_checkpoint, torch_dtype=torch.float16, use_safetensors=use_safetensors)\n",
        "        pipe = pipe.to(current_device)\n",
        "        pipe.safety_checker = None\n",
        "        if current_scheduler == \"ddpm\":\n",
        "          pipe.scheduler = ddpm.from_config(pipe.scheduler.config)\n",
        "        if current_scheduler == \"ddim\":\n",
        "          pipe.scheduler = ddim.from_config(pipe.scheduler.config)\n",
        "        if current_scheduler == \"pndm\":\n",
        "          pipe.scheduler = pndm.from_config(pipe.scheduler.config)\n",
        "        if current_scheduler == \"lms\":\n",
        "          pipe.scheduler = lms.from_config(pipe.scheduler.config)\n",
        "        if current_scheduler == \"euler\":\n",
        "          pipe.scheduler = euler.from_config(pipe.scheduler.config)\n",
        "        if current_scheduler == \"euler_anc\":\n",
        "          pipe.scheduler = euler_anc.from_config(pipe.scheduler.config)\n",
        "        if current_scheduler == \"dpm\":\n",
        "          pipe.scheduler = dpm.from_config(pipe.scheduler.config)\n",
        "        print(\"Everything is ready to go :)\")\n",
        "        clear_output()\n",
        "        previous_mode = mode\n",
        "        previous_checkpoint = current_checkpoint\n",
        "    if use_lora:\n",
        "      if lora_path != previous_lora_path and lora_url == \"\":\n",
        "        if pipe:\n",
        "          pipe.unload_lora_weights()\n",
        "        print(\"Installing LoRA model...\")\n",
        "        pipe.load_lora_weights(lora_path, weight_name=weight_name)\n",
        "        print(\"Everything is ready to go :)\")\n",
        "        previous_lora_path = lora_path\n",
        "      if lora_url != previous_lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "        if pipe:\n",
        "          pipe.unload_lora_weights()\n",
        "        response = requests.get(lora_url, stream=True)\n",
        "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
        "        block_size = 100000\n",
        "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "\n",
        "        if 'content-disposition' in response.headers:\n",
        "          content_disposition = response.headers['content-disposition']\n",
        "          filename_index = content_disposition.find('filename=')\n",
        "          if filename_index != -1:\n",
        "            filename = content_disposition[filename_index + len('filename='):]\n",
        "            filename = unquote(filename)\n",
        "            filename = filename.strip('\"')\n",
        "            downloaded_lora_name = filename\n",
        "        if not downloaded_lora_name:\n",
        "          downloaded_lora_name = os.path.basename(lora_url)\n",
        "\n",
        "        downloaded_lora_basename, _ = os.path.splitext(downloaded_lora_name)\n",
        "        with open(downloaded_lora_name, 'wb') as file:\n",
        "          for data in response.iter_content(block_size):\n",
        "            progress_bar.update(len(data))\n",
        "            file.write(data)\n",
        "        progress_bar.close()\n",
        "        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "          print(\"Model loading error :(\")\n",
        "        pipe.load_lora_weights(downloaded_lora_name)\n",
        "        previous_lora_url = lora_url\n",
        "        print(\"Everything is ready to go :)\")\n",
        "\n",
        "\n",
        "# print(\"\\nList of models used during the session:\")\n",
        "# model_fields = []\n",
        "# for i, model_name in enumerate(used_models_array, start=1):\n",
        "#     if model_name:\n",
        "#       model_field = widgets.Text(\n",
        "#           value=model_name,\n",
        "#           description=f'Model {i-1}:',\n",
        "#           disabled=False,\n",
        "#           layout=widgets.Layout(width=\"auto\")\n",
        "#       )\n",
        "#       model_fields.append(model_field)\n",
        "\n",
        "# display(widgets.VBox(model_fields))\n",
        "\n",
        "# if len(used_models_array) == 1:\n",
        "#   print(\"So far, the list of models is empty :(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVG80_y8vCpH"
      },
      "outputs": [],
      "source": [
        "# @title Setting up the generation parameters: { form-width: \"10%\", display-mode: \"form\" }\n",
        "# Здесь указываем переменные для пайплайна Stable Diffusion\n",
        "# Дефолтный промпт для лучшего качества: (8k, ultra realistic, highly detailed, cinematic lighting)\n",
        "\n",
        "# @markdown Prompt:\n",
        "prompt = \"werewolf, forest, night\" # @param {type:\"string\"}\n",
        "# @markdown Negative prompt:\n",
        "negs = \"\" # @param {type:\"string\"}\n",
        "# @markdown Generated image style:\n",
        "styles = \"No style\" # @param [\"No style\", \"ran4erep's style\", \"3D model\", \"Analog film\", \"Anime\", \"Cinematic\", \"Comic book\", \"Plasticine\", \"Digital art\", \"Fantasy\", \"Isometric\", \"Sketch\", \"Low-poly\", \"Neonpunk\", \"Origami\", \"Photo\", \"Pixel art\", \"Texture\", \"Abstraction\", \"Abstract Expressionism\", \"Hyperrealism\", \"Pop-art\", \"Renaissance\", \"Steampunk\", \"Surrealism\", \"Futurism\", \"Syntwave-futurism\", \"Retro-futurism\", \"Sci-Fi\", \"Vaporwave futurism\", \"Dystopian\", \"Gothic\", \"Grunge\", \"Horror\", \"Lovecraftian\", \"Creepy\", \"Minimalism\", \"Noir\", \"Long exposure\", \"Tilt-Shift\", \"iPhone photo\"]\n",
        "# @markdown ---\n",
        "\n",
        "def latents_to_rgb(latents):\n",
        "    weights = (\n",
        "        (60, -60, 25, -70),\n",
        "        (60,  -5, 15, -50),\n",
        "        (60,  10, -5, -35)\n",
        "    )\n",
        "\n",
        "    weights_tensor = torch.t(torch.tensor(weights, dtype=latents.dtype).to(latents.device))\n",
        "    biases_tensor = torch.tensor((150, 140, 130), dtype=latents.dtype).to(latents.device)\n",
        "    rgb_tensor = torch.einsum(\"...lxy,lr -> ...rxy\", latents, weights_tensor) + biases_tensor.unsqueeze(-1).unsqueeze(-1)\n",
        "    image_array = rgb_tensor.clamp(0, 255)[0].byte().cpu().numpy()\n",
        "    image_array = image_array.transpose(1, 2, 0)  # Change the order of dimensions\n",
        "\n",
        "    return Image.fromarray(image_array)\n",
        "\n",
        "def decode_tensors(pipe, step, timestep, callback_kwargs):\n",
        "  if step % 5 == 0:\n",
        "    latents = callback_kwargs[\"latents\"]\n",
        "\n",
        "    image = latents_to_rgb(latents)\n",
        "    image_resized = image.resize((412, 412), Image.NEAREST)\n",
        "    update_display(image_resized, display_id=display_handle.display_id)\n",
        "\n",
        "  return callback_kwargs\n",
        "\n",
        "if styles != \"No style\":\n",
        "  if styles == \"ran4erep's style\":\n",
        "    prompt = prompt + \", 8k, ultra realistic, highly detailed\"\n",
        "    negs = negs + \", canvas frame, cartoon, 3d, disfigured, bad art, deformed, extra limbs, close up, b&w, wierd colors, blurry, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, out of frame, ugly, extra limbs, bad anatomy, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, mutated hands, fused fingers, too many fingers, long neck, Photoshop, video game, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, mutation, mutated, extra limbs, extra legs, extra arms, disfigured, deformed, cross-eye, body out of frame, blurry, bad art, bad anatomy, 3d render\"\n",
        "  if styles == \"3D model\":\n",
        "    prompt = \"professional 3d model {\" + prompt + \"} . octane render, highly detailed, volumetric, dramatic lighting\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, low poly, blurry, painting\"\n",
        "  if styles == \"Analog film\":\n",
        "    prompt = \"analog film photo {\" + prompt + \"} . faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage\"\n",
        "    negs   = negs + \", painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\"\n",
        "  if styles == \"Anime\":\n",
        "    prompt = \"anime artwork {\" + prompt + \"} . anime style, key visual, vibrant, studio anime, highly detailed\"\n",
        "    negs   = negs + \", photo, deformed, black and white, realism, disfigured, low contrast\"\n",
        "  if styles == \"Cinematic\":\n",
        "    prompt = \"cinematic film still {\" + prompt + \"} . shallow depth of field, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\"\n",
        "    negs   = negs + \", anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\"\n",
        "  if styles == \"Comic book\":\n",
        "    prompt = \"comic {\" + prompt + \"} . graphic illustration, comic art, graphic novel art, vibrant, highly detailed\"\n",
        "    negs   = negs + \", photograph, deformed, glitch, noisy, realistic, stock photo\"\n",
        "  if styles == \"Plasticine\":\n",
        "    prompt = \"play-doh style {\" + prompt + \"} . sculpture, clay art, centered composition, Claymation\"\n",
        "    negs   = negs + \", sloppy, messy, grainy, highly detailed, ultra textured, photo\"\n",
        "  if styles == \"Digital art\":\n",
        "    prompt = \"concept art {\" + prompt + \"} . digital artwork, illustrative, painterly, matte painting, highly detailed\"\n",
        "    negs   = negs + \", photo, photorealistic, realism, ugly\"\n",
        "  if styles == \"Fantasy\":\n",
        "    prompt = \"ethereal fantasy concept art of {\" + prompt + \"} . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy\"\n",
        "    negs   = negs + \", photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white\"\n",
        "  if styles == \"Isometric\":\n",
        "    prompt = \"isometric style {\" + prompt + \"} . vibrant, beautiful, crisp, detailed, ultra detailed, intricate\"\n",
        "    negs   = negs + \", deformed, mutated, ugly, disfigured, blur, blurry, noise, noisy, realistic, photographic\"\n",
        "  if styles == \"Sketch\":\n",
        "    prompt = \"line art drawing {\" + prompt + \"} . professional, sleek, modern, minimalist, graphic, line art, vector graphics\"\n",
        "    negs   = negs + \", anime, photorealistic, 35mm film, deformed, glitch, blurry, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, mutated, realism, realistic, impressionism, expressionism, oil, acrylic\"\n",
        "  if styles == \"Low-poly\":\n",
        "    prompt = \"low-poly style {\" + prompt + \"} . low-poly game art, polygon mesh, jagged, blocky, wireframe edges, centered composition\"\n",
        "    negs   = negs + \", noisy, sloppy, messy, grainy, highly detailed, ultra textured, photo\"\n",
        "  if styles == \"Neonpunk\":\n",
        "    prompt = \"neonpunk style {\" + prompt + \"} . cyberpunk, vaporwave, neon, vibes, vibrant, stunningly beautiful, crisp, detailed, sleek, ultramodern, magenta highlights, dark purple shadows, high contrast, cinematic, ultra detailed, intricate, professional\"\n",
        "    negs   = negs + \", painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\"\n",
        "  if styles == \"Origami\":\n",
        "    prompt = \"origami style {\" + prompt + \"} . paper art, pleated paper, folded, origami art, pleats, cut and fold, centered composition\"\n",
        "    negs   = negs + \", noisy, sloppy, messy, grainy, highly detailed, ultra textured, photo\"\n",
        "  if styles == \"Photo\":\n",
        "    prompt = \"cinematic photo {\" + prompt + \"} . 35mm photograph, film, bokeh, professional, 4k, highly detailed\"\n",
        "    negs   = negs + \", drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\"\n",
        "  if styles == \"Pixel art\":\n",
        "    prompt = \"pixel-art {\" + prompt + \"} . low-res, blocky, pixel art style, 8-bit graphics\"\n",
        "    negs   = negs + \", sloppy, messy, blurry, noisy, highly detailed, ultra textured, photo, realistic\"\n",
        "  if styles == \"Texture\":\n",
        "    prompt = \"texture {\" + prompt + \"} top down close-up\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry\"\n",
        "  if styles == \"Abstraction\":\n",
        "    prompt = \"abstract style {\" + prompt + \"} . non-representational, colors and shapes, expression of feelings, imaginative, highly detailed\"\n",
        "    negs   = negs + \", realistic, photographic, figurative, concrete\"\n",
        "  if styles == \"Abstract Expressionism\":\n",
        "    prompt = \"abstract expressionist painting {\" + prompt + \"} . energetic brushwork, bold colors, abstract forms, expressive, emotional\"\n",
        "    negs   = negs + \", realistic, photorealistic, low contrast, plain, simple, monochrome\"\n",
        "  if styles == \"Hyperrealism\":\n",
        "    prompt = \"hyperrealistic art {\" + prompt + \"} . extremely high-resolution details, photographic, realism pushed to extreme, fine texture, incredibly lifelike\"\n",
        "    negs   = negs + \", simplified, abstract, unrealistic, impressionistic, low resolution\"\n",
        "  if styles == \"Pop-art\":\n",
        "    prompt = \"pop Art style {\" + prompt + \"} . bright colors, bold outlines, popular culture themes, ironic or kitsch\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, minimalist\"\n",
        "  if styles == \"Renaissance\":\n",
        "    prompt = \"renaissance style {\" + prompt + \"} . realistic, perspective, light and shadow, religious or mythological themes, highly detailed\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, modernist, minimalist, abstract\"\n",
        "  if styles == \"Steampunk\":\n",
        "    prompt = \"steampunk style {\" + prompt + \"} . antique, mechanical, brass and copper tones, gears, intricate, detailed\"\n",
        "    negs   = negs + \", deformed, glitch, noisy, low contrast, anime, photorealistic\"\n",
        "  if styles == \"Surrealism\":\n",
        "    prompt = \"surrealist art {\" + prompt + \"} . dreamlike, mysterious, provocative, symbolic, intricate, detailed\"\n",
        "    negs   = negs + \", anime, photorealistic, realistic, deformed, glitch, noisy, low contrast\"\n",
        "  if styles == \"Futurism\":\n",
        "    prompt = \"futuristic style {\" + prompt + \"} . sleek, modern, ultramodern, high tech, detailed\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vintage, antique\"\n",
        "  if styles == \"Syntwave-futurism\":\n",
        "    prompt = \"retro cyberpunk {\" + prompt + \"} . 80’s inspired, synthwave, neon, vibrant, detailed, retro futurism\"\n",
        "    negs   = negs + \", modern, desaturated, black and white, realism, low contrast\"\n",
        "  if styles == \"Retro-futurism\":\n",
        "    prompt = \"retro-futuristic {\" + prompt + \"} . vintage sci-fi, 50s and 60s style, atomic age, vibrant, highly detailed\"\n",
        "    negs   = negs + \", contemporary, realistic, rustic, primitive\"\n",
        "  if styles == \"Sci-Fi\":\n",
        "    prompt = \"sci-fi style {\" + prompt + \"} . futuristic, technological, alien worlds, space themes, advanced civilizations\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, historical, medieval\"\n",
        "  if styles == \"Vaporwave futurism\":\n",
        "    prompt = \"vaporwave style {\" + prompt + \"} . retro aesthetic, cyberpunk, vibrant, neon colors, vintage 80s and 90s style, highly detailed\"\n",
        "    negs   = negs + \", monochrome, muted colors, realism, rustic, minimalist, dark\"\n",
        "  if styles == \"Dystopian\":\n",
        "    prompt = \"dystopian style {\" + prompt + \"} . bleak, post-apocalyptic, somber, dramatic, highly detailed\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, cheerful, optimistic, vibrant, colorful\"\n",
        "  if styles == \"Gothic\":\n",
        "    prompt = \"gothic style {\" + prompt + \"} . dark, mysterious, haunting, dramatic, ornate, detailed\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, cheerful, optimistic\"\n",
        "  if styles == \"Grunge\":\n",
        "    prompt = \"grunge style {\" + prompt + \"} . textured, distressed, vintage, edgy, punk rock vibe, dirty, noisy\"\n",
        "    negs   = negs + \", smooth, clean, minimalist, sleek, modern, photorealistic\"\n",
        "  if styles == \"Horror\":\n",
        "    prompt = \"horror-themed {\" + prompt + \"} . eerie, unsettling, dark, spooky, suspenseful, grim, highly detailed\"\n",
        "    negs   = negs + \", cheerful, bright, vibrant, light-hearted, cute\"\n",
        "  if styles == \"Lovecraftian\":\n",
        "    prompt = \"lovecraftian horror {\" + prompt + \"} . eldritch, cosmic horror, unknown, mysterious, surreal, highly detailed\"\n",
        "    negs   = negs + \", light-hearted, mundane, familiar, simplistic, realistic\"\n",
        "  if styles == \"Creepy\":\n",
        "    prompt = \"macabre style {\" + prompt + \"} . dark, gothic, grim, haunting, highly detailed\"\n",
        "    negs   = negs + \", bright, cheerful, light-hearted, cartoonish, cute\"\n",
        "  if styles == \"Minimalism\":\n",
        "    prompt = \"minimalist style {\" + prompt + \"} . simple, clean, uncluttered, modern, elegant\"\n",
        "    negs   = negs + \", ornate, complicated, highly detailed, cluttered, disordered, messy, noisy\"\n",
        "  if styles == \"Noir\":\n",
        "    prompt = \"film noir style {\" + prompt + \"} . monochrome, high contrast, dramatic shadows, 1940s style, mysterious, cinematic\"\n",
        "    negs   = negs + \", ugly, deformed, noisy, blurry, low contrast, realism, photorealistic, vibrant, colorful\"\n",
        "  if styles == \"Long exposure\":\n",
        "    prompt = \"long exposure photo of {\" + prompt + \"} . Blurred motion, streaks of light, surreal, dreamy, ghosting effect, highly detailed\"\n",
        "    negs   = negs + \", static, noisy, deformed, shaky, abrupt, flat, low contrast\"\n",
        "  if styles == \"Tilt-Shift\":\n",
        "    prompt = \"tilt-shift photo of {\" + prompt + \"} . selective focus, miniature effect, blurred background, highly detailed, vibrant, perspective control\"\n",
        "    negs   = negs + \", blurry, noisy, deformed, flat, low contrast, unrealistic, oversaturated, underexposed\"\n",
        "  if styles == \"iPhone photo\":\n",
        "    prompt = \"iphone photo {\" + prompt + \"} . large depth of field, deep depth of field, highly detailed\"\n",
        "    negs   = negs + \", drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly, shallow depth of field, bokeh\"\n",
        "# @markdown For which Stable Diffusion version you have loaded the model? (It's important for workability tokens weight, otherwise there will be an error)\n",
        "sd_type = \"Stable Diffusion 1.5/2.0\" # @param [\"Stable Diffusion 1.5/2.0\", \"Stable Diffusion XL (Turbo)\"]\n",
        "# @markdown Token weight for SD 1.5 is written as + or - after the word inside the token: \"cat playing with ball+++++\", \"cat+ playing with ball\"\n",
        "# @markdown\n",
        "# @markdown The weight for SDXL is written as a fractional number after brackets with a word or even a whole token inside it: \"cat (playing with ball)1.5\"\n",
        "# @markdown\n",
        "# @markdown ---\n",
        "# @markdown Width:\n",
        "width = 512 # @param {type:\"number\"}\n",
        "# @markdown Height:\n",
        "height = 512 # @param {type:\"number\"}\n",
        "# @markdown Number of images:\n",
        "images_count = 1 # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "# @markdown Steps:\n",
        "steps = 25 # @param {type:\"number\"}\n",
        "# @markdown Guidance scale:\n",
        "gscale = 7.5 # @param {type:\"number\"}\n",
        "# @markdown Current scheduler:\n",
        "current_scheduler = \"euler\" # @param [\"ddpm\", \"ddim\", \"pndm\", \"lms\", \"euler_anc\", \"euler\", \"dpm\"] {type:\"string\"}\n",
        "\n",
        "# @markdown Use random seed?\n",
        "randomness = True # @param {type:\"boolean\"}\n",
        "# @markdown Seed:\n",
        "seed = 5107167905980673 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq2nSTBmoWq-"
      },
      "source": [
        "### Run Stable Diffusion\n",
        "Do not run this block of code in its entirety. Expand it and you will see the different modes of Stable Diffusion. Run the mode you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl8kcwyOqfd_"
      },
      "outputs": [],
      "source": [
        "# @title Text2Image: { form-width: \"10%\", display-mode: \"form\" }\n",
        "# @markdown\n",
        "# @markdown <-- Start generation\n",
        "# Генерация картинки\n",
        "# @markdown\n",
        "# @markdown Show diffusion in real time?\n",
        "show_diffusion = True # @param {type:\"boolean\"}\n",
        "\n",
        "ddpm = DDPMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "ddim = DDIMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "pndm = PNDMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "lms = LMSDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "euler_anc = EulerAncestralDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "euler = EulerDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "dpm = DPMSolverMultistepScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "\n",
        "run_chkpt(current_checkpoint, lora_path, use_lora, \"txt2img\")\n",
        "\n",
        "if sd_type == \"Stable Diffusion 1.5/2.0\":\n",
        "  compel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "  prompt_embeds = compel_proc(prompt)\n",
        "\n",
        "if sd_type == \"Stable Diffusion XL (Turbo)\":\n",
        "  compel = Compel(\n",
        "    tokenizer=[pipe.tokenizer, pipe.tokenizer_2] ,\n",
        "    text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "    requires_pooled=[False, True]\n",
        "  )\n",
        "  conditioning, pooled = compel(prompt)\n",
        "\n",
        "#####################################################################\n",
        "# Информация о текущей генерации\n",
        "output = [\n",
        "    \"# Current Stable Diffusion model: \" + current_checkpoint,\n",
        "]\n",
        "if use_lora:\n",
        "    if lora_url == \"\" and lora_path and weight_name:\n",
        "      output.append(\"# Current LoRA model: \" + lora_path)\n",
        "    elif lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "      output.append(\"# Current LoRA model: \" + downloaded_lora_basename)\n",
        "output.extend([\n",
        "    \"# Current style: \" + styles,\n",
        "    \"# Prompt: \" + prompt,\n",
        "    \"# Negative prompt: \" + negs,\n",
        "    \"# Resolution: \" + str(width) + \"x\" + str(height),\n",
        "    \"# Number of generated images: \" + str(images_count)\n",
        "])\n",
        "max_length_string = max(output, key=len)\n",
        "\n",
        "print(\"#\" * len(max_length_string) )\n",
        "for item in output:\n",
        "  print(item)\n",
        "\n",
        "######################################################################\n",
        "if not randomness:\n",
        "  generator = torch.Generator(current_device).manual_seed(seed)\n",
        "  print(\"# Current generation's seed: \" + str(seed) + \"\\n\" + (\"#\" * len(max_length_string)) )\n",
        "elif randomness:\n",
        "  current_seed = torch.Generator(current_device).seed()\n",
        "  generator = torch.Generator(current_device).manual_seed(current_seed)\n",
        "  print(\"# Current generation's seed: \" + str(current_seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "\n",
        "\n",
        "#result = pipe(prompt=prompt, height=height, width=width, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator, num_images_per_prompt=images_count)\n",
        "\n",
        "if sd_type == \"Stable Diffusion 1.5/2.0\":\n",
        "  if show_diffusion:\n",
        "    torch.cuda.empty_cache()\n",
        "    result = pipe(prompt_embeds=prompt_embeds, callback_on_step_end=decode_tensors, callback_on_step_end_tensor_inputs=[\"latents\"], height=height, width=width, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator, num_images_per_prompt=images_count)\n",
        "  else:\n",
        "    torch.cuda.empty_cache()\n",
        "    result = pipe(prompt_embeds=prompt_embeds, height=height, width=width, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator, num_images_per_prompt=images_count)\n",
        "\n",
        "if sd_type == \"Stable Diffusion XL (Turbo)\":\n",
        "  if show_diffusion:\n",
        "    torch.cuda.empty_cache()\n",
        "    result = pipe(prompt_embeds=conditioning, pooled_prompt_embeds=pooled, callback_on_step_end=decode_tensors, callback_on_step_end_tensor_inputs=[\"latents\"], height=height, width=width, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator, num_images_per_prompt=images_count)\n",
        "  else:\n",
        "    torch.cuda.empty_cache()\n",
        "    result = pipe(prompt_embeds=conditioning, pooled_prompt_embeds=pooled, height=height, width=width, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator, num_images_per_prompt=images_count)\n",
        "\n",
        "# Вывод и сохрарнение результата\n",
        "\n",
        "if use_gdrive:\n",
        "  today = str(date.today())\n",
        "  if not os.path.exists(\"/content/gdrive/MyDrive/SDOutput/\" + today):\n",
        "    os.makedirs(\"/content/gdrive/MyDrive/SDOutput/\" + today, exist_ok=True)\n",
        "  for i in range(images_count):\n",
        "    time = datetime.now()\n",
        "    time = str(time.strftime(\"%H-%M-%S\"))\n",
        "    file_name = time + \"_\" + \"seed_\" + str(current_seed) + \"_image\" + str(i) +\".png\"\n",
        "    file_name = \"/content/gdrive/MyDrive/SDOutput/\" + today + \"/\" + file_name\n",
        "    images = make_image_grid(result.images, rows=1, cols=images_count)\n",
        "    result.images[i].save(file_name)\n",
        "    print(\"Saved as \" + file_name)\n",
        "\n",
        "clear_output()\n",
        "#update_display(result, display_id=display_handle.display_id)\n",
        "print(\"#\" * len(max_length_string) )\n",
        "print(\"# Current Stable Diffusion model: \" + current_checkpoint)\n",
        "if use_lora:\n",
        "  if lora_url == \"\" and lora_path and weight_name:\n",
        "    output.append(\"# Current LoRA model: \" + lora_path)\n",
        "  elif lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "    output.append(\"# Current LoRA model: \" + downloaded_lora_basename)\n",
        "print(\"# Current style: \" + styles)\n",
        "print(\"# Prompt: \" + prompt)\n",
        "print(\"# Negative prompt: \" + negs)\n",
        "print(\"# Resolution: \" + str(width) + \"x\" + str(height))\n",
        "print(\"# Number of generated images: \" + str(images_count))\n",
        "if not randomness:\n",
        "  print(\"# Current generation's seed: \" + str(seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "elif randomness:\n",
        "  print(\"# Current generation's seed: \" + str(current_seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "make_image_grid(result.images, rows=1, cols=images_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FtAIUUMVO8hS"
      },
      "outputs": [],
      "source": [
        "# @title Image loading for Image2Image module\n",
        "print(\"Pick source image: \")\n",
        "init_image = files.upload()\n",
        "image_key = list(init_image.keys())[0]\n",
        "image_data = init_image[image_key]\n",
        "img = Image.open(io.BytesIO(image_data))\n",
        "img_width, img_height = img.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kbebpfrLohQX"
      },
      "outputs": [],
      "source": [
        "# @title Image2Image:\n",
        "# @markdown <-- Start generation\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Strength:\n",
        "\n",
        "# @markdown The lower the value, the closer the generation will be to the image you uploaded. The higher the value, the stronger the neural network will be creative.\n",
        "\n",
        "img2img_strength = 0.3 # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "\n",
        "# @markdown Strength must be combined with steps. The total number of steps for noise generation is calculated by the formula (Steps * Strength).\n",
        "\n",
        "# @markdown Steps:\n",
        "steps = 25 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "# @markdown Show diffusion in real time?\n",
        "show_diffusion = True # @param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "\n",
        "ddpm = DDPMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "ddim = DDIMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "pndm = PNDMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "lms = LMSDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "euler_anc = EulerAncestralDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "euler = EulerDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "dpm = DPMSolverMultistepScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "\n",
        "run_chkpt(current_checkpoint, lora_path, use_lora, \"img2img\")\n",
        "\n",
        "\n",
        "if init_image:\n",
        "\n",
        "  if sd_type == \"Stable Diffusion 1.5/2.0\":\n",
        "    compel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "    prompt_embeds = compel_proc(prompt)\n",
        "\n",
        "  if sd_type == \"Stable Diffusion XL (Turbo)\":\n",
        "    compel = Compel(\n",
        "      tokenizer=[pipe.tokenizer, pipe.tokenizer_2] ,\n",
        "      text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "      returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "      requires_pooled=[False, True]\n",
        "    )\n",
        "    conditioning, pooled = compel(prompt)\n",
        "\n",
        "  output = [\n",
        "      \"# Current Stable Diffusion model: \" + current_checkpoint,\n",
        "  ]\n",
        "  if lora_url == \"\" and lora_path and weight_name:\n",
        "    output.append(\"# Current LoRA model: \" + lora_path)\n",
        "  elif lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "    output.append(\"# Current LoRA model: \" + downloaded_lora_basename)\n",
        "  output.extend([\n",
        "      \"# Current style: \" + styles,\n",
        "      \"# Prompt: \" + prompt,\n",
        "      \"# Negative prompt: \" + negs,\n",
        "      \"# Resolution: \" + str(img_width) + \"x\" + str(img_height),\n",
        "      \"# Strength: \" + str(img2img_strength)\n",
        "  ])\n",
        "  max_length_string = max(output, key=len)\n",
        "\n",
        "  print(\"#\" * len(max_length_string) )\n",
        "  for item in output:\n",
        "    print(item)\n",
        "\n",
        "  if not randomness:\n",
        "    generator = torch.Generator(current_device).manual_seed(seed)\n",
        "    print(\"# Current generation's seed: \" + str(seed) + \"\\n\" + (\"#\" * len(max_length_string)) )\n",
        "  elif randomness:\n",
        "    current_seed = torch.Generator(current_device).seed()\n",
        "    generator = torch.Generator(current_device).manual_seed(current_seed)\n",
        "    print(\"# Current generation's seed: \" + str(current_seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "\n",
        "  if sd_type == \"Stable Diffusion 1.5/2.0\":\n",
        "    if show_diffusion:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=prompt_embeds, callback_on_step_end=decode_tensors, callback_on_step_end_tensor_inputs=[\"latents\"], image=img, strength = img2img_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "    else:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=prompt_embeds, image=img, strength = img2img_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "\n",
        "  if sd_type == \"Stable Diffusion XL (Turbo)\":\n",
        "    if show_diffusion:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=conditioning, pooled_prompt_embeds=pooled, callback_on_step_end=decode_tensors, callback_on_step_end_tensor_inputs=[\"latents\"], image=img, strength = img2img_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "    else:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=conditioning, pooled_prompt_embeds=pooled, image=img, strength = img2img_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "\n",
        "  clear_output()\n",
        "  print(\"#\" * len(max_length_string) )\n",
        "  print(\"# Current Stable Diffusion model: \" + current_checkpoint)\n",
        "  if use_lora:\n",
        "    if lora_url == \"\" and lora_path and weight_name:\n",
        "      output.append(\"# Current LoRA model: \" + lora_path)\n",
        "    elif lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "      output.append(\"# Current LoRA model: \" + downloaded_lora_basename)\n",
        "  print(\"# Current style: \" + styles)\n",
        "  print(\"# Prompt: \" + prompt)\n",
        "  print(\"# Negative prompt: \" + negs)\n",
        "  print(\"# Resolution: \" + str(img_width) + \"x\" + str(img_height))\n",
        "  print(\"# Strength: \" + str(img2img_strength))\n",
        "  if not randomness:\n",
        "    print(\"# Current generation's seed: \" + str(seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "  elif randomness:\n",
        "    print(\"# Current generation's seed: \" + str(current_seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "\n",
        "  if use_gdrive:\n",
        "    today = str(date.today())\n",
        "    if not os.path.exists(\"/content/gdrive/MyDrive/SDOutput/\" + today):\n",
        "      os.makedirs(\"/content/gdrive/MyDrive/SDOutput/\" + today, exist_ok=True)\n",
        "    time = datetime.now()\n",
        "    time = str(time.strftime(\"%H-%M-%S\"))\n",
        "    file_name = time + \"_\" + \"seed_\" + str(current_seed) + \"_image\" + \"_img2imged.png\"\n",
        "    file_name = \"/content/gdrive/MyDrive/SDOutput/\" + today + \"/\" + file_name\n",
        "    result.save(file_name)\n",
        "    print(\"Saved as \" + file_name)\n",
        "\n",
        "  display( make_image_grid([img, result], rows=1, cols=2) )\n",
        "else:\n",
        "  print(\"There is no image to generate from :(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "x0hV1I-eQTMB"
      },
      "outputs": [],
      "source": [
        "# @title Image loading for Inpainting module\n",
        "print(\"Pick source image: \")\n",
        "inpaint_image = files.upload()\n",
        "print(\"Pick mask image: \")\n",
        "inpaint_mask =  files.upload()\n",
        "\n",
        "inpaint_image_key = list(inpaint_image.keys())[0]\n",
        "inpaint_mask_image_key = list(inpaint_mask.keys())[0]\n",
        "inpaint_image_data = inpaint_image[inpaint_image_key]\n",
        "inpaint_mask_image_data = inpaint_mask[inpaint_mask_image_key]\n",
        "inpaint_image = Image.open(io.BytesIO(inpaint_image_data)).convert('RGB')\n",
        "inpaint_mask_image = Image.open(io.BytesIO(inpaint_mask_image_data)).convert('RGB')\n",
        "inpaint_image_width, inpaint_image_height = inpaint_image.size\n",
        "inpaint_image_mask_width, inpaint_image_mask_height = inpaint_mask_image.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ibVBEYWQo2jQ"
      },
      "outputs": [],
      "source": [
        "# @title Inpainting:\n",
        "# @markdown <-- Start generation\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown Strength:\n",
        "\n",
        "# @markdown The lower the value, the closer the generation will be to the image you uploaded. The higher the value, the stronger the neural network will be creative.\n",
        "\n",
        "inpaint_strength = 1 # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "\n",
        "# @markdown Strength must be combined with steps. The total number of steps for noise generation is calculated by the formula (Steps * Strength).\n",
        "\n",
        "# @markdown Steps:\n",
        "steps = 25 # @param {type:\"number\"}\n",
        "# @markdown ---\n",
        "# @markdown Show diffusion in real time?\n",
        "show_diffusion = True # @param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "# @markdown When drawing a mask, remember that black areas are ignored by the neural network, while white areas are not. Blurred white areas in masks is also supported.\n",
        "\n",
        "ddpm = DDPMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "ddim = DDIMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "pndm = PNDMScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "lms = LMSDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "euler_anc = EulerAncestralDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "euler = EulerDiscreteScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "dpm = DPMSolverMultistepScheduler.from_pretrained(current_checkpoint, subfolder=\"scheduler\")\n",
        "\n",
        "run_chkpt(current_checkpoint, lora_path, use_lora, \"inpainting\")\n",
        "\n",
        "if inpaint_image and inpaint_mask:\n",
        "\n",
        "  if sd_type == \"Stable Diffusion 1.5/2.0\":\n",
        "    compel_proc = Compel(tokenizer=pipe.tokenizer, text_encoder=pipe.text_encoder)\n",
        "    prompt_embeds = compel_proc(prompt)\n",
        "\n",
        "  if sd_type == \"Stable Diffusion XL (Turbo)\":\n",
        "    compel = Compel(\n",
        "      tokenizer=[pipe.tokenizer, pipe.tokenizer_2] ,\n",
        "      text_encoder=[pipe.text_encoder, pipe.text_encoder_2],\n",
        "      returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "      requires_pooled=[False, True]\n",
        "    )\n",
        "    conditioning, pooled = compel(prompt)\n",
        "\n",
        "  output = [\n",
        "      \"# Current Stable Diffusion model: \" + current_checkpoint,\n",
        "  ]\n",
        "  if lora_url == \"\" and lora_path and weight_name:\n",
        "    output.append(\"# Current LoRA model: \" + lora_path)\n",
        "  elif lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "    output.append(\"# Current LoRA model: \" + downloaded_lora_basename)\n",
        "  output.extend([\n",
        "      \"# Current style: \" + styles,\n",
        "      \"# Prompt: \" + prompt,\n",
        "      \"# Negative prompt: \" + negs,\n",
        "      \"# Source image resolution: \" + str(inpaint_image_width) + \"x\" + str(inpaint_image_height),\n",
        "      \"# Mask image resolution: \" + str(inpaint_image_mask_width) + \"x\" + str(inpaint_image_mask_height),\n",
        "      \"# Strength: \" + str(inpaint_strength)\n",
        "  ])\n",
        "  max_length_string = max(output, key=len)\n",
        "\n",
        "  print(\"#\" * len(max_length_string) )\n",
        "  for item in output:\n",
        "    print(item)\n",
        "\n",
        "  if not randomness:\n",
        "    generator = torch.Generator(current_device).manual_seed(seed)\n",
        "    print(\"# Current generation's seed: \" + str(seed) + \"\\n\" + (\"#\" * len(max_length_string)) )\n",
        "  elif randomness:\n",
        "    current_seed = torch.Generator(current_device).seed()\n",
        "    generator = torch.Generator(current_device).manual_seed(current_seed)\n",
        "    print(\"# Current generation's seed: \" + str(current_seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "\n",
        "  if sd_type == \"Stable Diffusion 1.5/2.0\":\n",
        "    if show_diffusion:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=prompt_embeds, callback_on_step_end=decode_tensors, callback_on_step_end_tensor_inputs=[\"latents\"], image=inpaint_image, mask_image=inpaint_mask_image, width=inpaint_image_width, height=inpaint_image_height, strength = inpaint_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "    else:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=prompt_embeds, image=inpaint_image, mask_image=inpaint_mask_image, width=inpaint_image_width, height=inpaint_image_height, strength = inpaint_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "\n",
        "  if sd_type == \"Stable Diffusion XL (Turbo)\":\n",
        "    if show_diffusion:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=conditioning, pooled_prompt_embeds=pooled, callback_on_step_end=decode_tensors, callback_on_step_end_tensor_inputs=[\"latents\"], image=inpaint_image, mask_image=inpaint_mask_image, width=inpaint_image_width, height=inpaint_image_height, strength = inpaint_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "    else:\n",
        "      torch.cuda.empty_cache()\n",
        "      result = pipe(prompt_embeds=conditioning, pooled_prompt_embeds=pooled, image=inpaint_image, mask_image=inpaint_mask_image, width=inpaint_image_width, height=inpaint_image_height, strength = inpaint_strength, num_inference_steps=steps, guidance_scale=gscale, negative_prompt=negs, generator=generator).images[0]\n",
        "\n",
        "  clear_output()\n",
        "  print(\"#\" * len(max_length_string) )\n",
        "  print(\"# Current Stable Diffusion model: \" + current_checkpoint)\n",
        "  if use_lora:\n",
        "    if lora_url == \"\" and lora_path and weight_name:\n",
        "      output.append(\"# Current LoRA model: \" + lora_path)\n",
        "    elif lora_url and lora_path == \"\" and weight_name == \"\":\n",
        "      output.append(\"# Current LoRA model: \" + downloaded_lora_basename)\n",
        "  print(\"# Current style: \" + styles)\n",
        "  print(\"# Prompt: \" + prompt)\n",
        "  print(\"# Negative prompt: \" + negs)\n",
        "  print(\"# Source image resolution: \" + str(inpaint_image_width) + \"x\" + str(inpaint_image_height) )\n",
        "  print(\"# Mask image resolution: \" + str(inpaint_image_mask_width) + \"x\" + str(inpaint_image_mask_height) )\n",
        "  print(\"# Strength: \" + str(inpaint_strength))\n",
        "  if not randomness:\n",
        "    print(\"# Current generation's seed: \" + str(seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "  elif randomness:\n",
        "    print(\"# Current generation's seed: \" + str(current_seed) + \"\\n\" + (\"#\" * len(max_length_string) ))\n",
        "\n",
        "  if use_gdrive:\n",
        "    today = str(date.today())\n",
        "    if not os.path.exists(\"/content/gdrive/MyDrive/SDOutput/\" + today):\n",
        "      os.makedirs(\"/content/gdrive/MyDrive/SDOutput/\" + today, exist_ok=True)\n",
        "    time = datetime.now()\n",
        "    time = str(time.strftime(\"%H-%M-%S\"))\n",
        "    file_name = time + \"_\" + \"seed_\" + str(current_seed) + \"_image\" + \"_inpainted.png\"\n",
        "    file_name = \"/content/gdrive/MyDrive/SDOutput/\" + today + \"/\" + file_name\n",
        "    result.save(file_name)\n",
        "    print(\"Saved as \" + file_name)\n",
        "\n",
        "  display( make_image_grid([inpaint_image, inpaint_mask_image, result], rows=1, cols=3) )\n",
        "else:\n",
        "  print(\"You have canceled the image uploading :(\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}